#! /usr/bin/env python3
# -*- mode: Python; -*-

from vrtargslib import trans_args, trans_main

from vrtnamelib import isnames, namelist, nameindices

from vrtdatalib import asrecord

def parsearguments():
    description = '''

    Mend sentences that were torn and then annotated with dependency
    syntax in CoNLL'09 format. Remove sen.ten.ce tags and sentence-tag
    pairs within sen.ten.ce, adjust numbering of tokens and heads in
    further shreds, pointing roots (relation ROOT and head 0) of
    further shreds to the root of the first shred (as its dep).

    '''
    parser = trans_args(description = description)

    parser.add_argument('--id', default = 'id',
                        help = '''

                        token id, a 1-based counter within sentence
                        (default: id)

                        ''')

    parser.add_argument('--rel', default = 'rel',
                        help = '''

                        dependency relation (default: rel)

                        ''')

    parser.add_argument('--head', default = 'head',
                        help = '''

                        dependency head, either id of another token
                        or 0 (default: head)

                        ''')

    args = parser.parse_args()
    args.prog = parser.prog
    return args

def main(args, ins, ous):
    
    def issome(line): return not line.isspace()

    idix, relix, headix = None, None, None
    keep = True
    look = False
    for line in filter(issome, ins):

        # outside <sen.ten.ce>, keep everything;
        # inside <sen.ten.ce>, drop sentence tags
        # but look for a first sentence start tag;
        # at </sen.ten.ce> close the <sentence>
        # (unless still looking for it to start)

        if isnames(line):
            print(line, end = '', file = ous)
            idix, relix, headix = nameindices(namelist(line),
                                              args.id,
                                              args.rel,
                                              args.head)
            continue

        if line.startswith(('<sen.ten.ce>', '<sen.ten.ce ')):
            keep = False
            look = True
            step, root, record = 0, None, None
            continue

        if line.startswith('</sen.ten.ce>'):
            look or print('</sentence>', file = ous)
            keep = True 
            look = False
            step, root, record = 0, None, None
            continue

        if line.startswith(('<sentence>', '<sentence ')):
            (keep or look) and print(line, end = '', file = ous)
            if look: look = False
            continue

        if line.startswith('</sentence>'):
            if keep:
                print(line, end = '', file = ous)
            else:
                # record is last token in a shred in a sentence being
                # mended, already adjusted to have its final id
                step = int(record[idix])
            continue

        if line.startswith('<'):
            print(line, end = '', file = ous)
            continue

        # line is a token

        if keep:
            print(line, end = '', file = ous)
            continue

        # line is a token in a sentence being mended

        record = asrecord(line)

        if record[headix] == '0':
            # record[relix] == 'ROOT'
            if step == 0:
                # root of first shred
                root = record[idix]
            else:
                # root of subsequent shred
                record[idix] = str(int(record[idix]) + step)
                record[relix] = 'dep'
                record[headix] = root
        elif step == 0:
            # in first shred
            pass
        else:
            # in subsequent shred
            record[idix] = str(int(record[idix]) + step)
            record[headix] = str(int(record[headix]) + step)

        print(*record, sep = '\t', file = ous)

if __name__ == '__main__':
    trans_main(parsearguments(), main)
