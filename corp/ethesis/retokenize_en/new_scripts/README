# Get source packages from IDA and extract them:

E-thesis_gradut_TXT_2016-11-22.zip under directory
 E-thesis_gradut_TXT_2016-11-22/

E-thesis_vaitokset_TXT_2016-10-17.zip under directory
 E-thesis_vaitokset_TXT_2016-10-17/ 

# Then copy and rename files from the source packages:

./copy-and-rename-files.sh COPIED
for dir in COPIED/*; do mv $dir . ; done

# Then for each CORPUSDIR:
# todo: missing date, datefrom, dateto, timefrom and timeto in some metadata
#       (see missing_year.txt)

./get-text-and-metadata.sh CORPUSDIR TSVFILE VRTFILE TARGETDIR
  --skip-missing --min-words 1000 --min-english-words 50

# For each DIR in TARGETDIR/*, combine the text and metadata
# and split them into files containing up to 50 texts each:

text-and-metadata-to-tnpp.sh DIR 50

# Analyze each file in TARGETDIR/*/TEXT_*.TXT with TNPP

source venv-parser-neural/bin/activate
cat TEXTFILE | python3 full_pipeline_stream.py --gpu -1 --conf models_en_ewt/pipelines.yaml parse_plaintext > TNPPFILE

# Convert output from TNPP into VRT

cat TNPPFILE | ./conllu-to-vrt.pl > VRTFILE
vrt-keep -i -n 'word,id,lemma,upos,xpos,feats,head,deprel,deps,misc' VRTFILE
vrt-rename -i -m id=ref -m head=dephead -m feats=msd -m upos=pos VRTFILE
vrt-validate VRTFILE

# todo: add word and sentence count for each text as well as corpus shortname

# Compile the VRT files for korp
# For each SUBCORPUS, concatenate the vrt files,
# skipping the initial comment lines:

echo '<!-- #vrt positional-attributes: word ref lemma pos xpos msd dephead deprel deps misc -->' > SUBCORPUS.vrt
for subcorpusfile in subcorpusfiles; do tail -n +2 $subcorpusfile >> SUBCORPUS.vrt done;

# Run korp-make for each SUBCORPUS

todo
