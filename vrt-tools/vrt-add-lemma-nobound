#! /usr/bin/env python3
# -*- mode: Python; -*-


"""
vrt-add-lemma-noboundaries

Add lemmas without compound boundaries to the input VRT with word
forms and lemmas with compound boundaries.
"""


import re
import sys

from itertools import chain

import vrtdatalib
import vrtnamelib

from vrtargsoolib import InputProcessor


class NoBoundaryLemmaAdder(InputProcessor):

    DESCRIPTION = """
    Add lemmas without compound boundaries to the input VRT with word
    forms and lemmas with compound boundaries.
    """
    ARGSPECS = [
        ('--mode (*simple-omorfi|omorfi|naive)',
         'use the specified mode for handling compound boundary markers in'
         ' lemmas: "simple-omorfi" handles compound boundary markers replacing'
         ' hyphens as produced by Omorfi; "omorfi" also tries to handle'
         ' lemmatized non-final compound parts; and "naive" simply removes'
         ' all compound boundary markers (unless thw word form contains them)'),
        ('--compound-boundary-marker=CHAR "|" -> boundary_char',
         'treat CHAR as the compound boundary marker'),
        ('--no-fix-spurious-boundaries',
         'do not correct "#" to "|" if the word form contains as many "|" as'
         ' the lemma contains "#", to fix a deficiency in a UD1 parser;'
         ' note that even without this option, the character is corrected only'
         ' in the added lemma without compound boundaries and only if the'
         ' compound boundary marker is "#"'),
        ('--only-wordform-hyphens',
         """with --mode=omorfi, do not add a hyphen between same
         vowels at a compound boundary if the word form does not have
         it. Omorfi overgenerates compound boundaries so that the word
         form "ongelmaanne" may get the lemma "ongelma#anne" even
         though prescriptively, the word form should have been
         "ongelma-anne". By default, --mode=omorfi then produces the
         lemma "ongelma-anne", but with this option, "ongelmaanne".
         """),
        ('--wordform-name=ATTR "word" -> word_attr',
         'use positional attribute ATTR as the word form'),
        ('--lemma-name=ATTR "lemma" -> lemma_attr',
         'use positional attribute ATTR as the lemma'),
        ('--noboundaries-name=ATTR "lemma_nobound" -> nobound_attr',
         'output the lemma without boundaries as positional attribute ATTR'),
        ('--insert-noboundaries-after-name|insert-after=ATTR "lemma"'
         ' -> insert_after_attr',
         'insert the lemma without boundaries after positional attribute ATTR'),
        ('--verbose',
         'log output to stderr'),
    ]

    # Lemma suffixes that are (typically) associated with a length change in
    # the word form, typically so that the word form is shorter (e.g. hevonen
    # -> hevos, häät -> hää). Some of these are peculiar to Omorfi, which may
    # give verb lemmas for noun derivatives.
    _lemma_suffix_len_changes = [
        ('nen', -2),	# hevonen -> hevos
        ('kset', -3),	# veljekset -> veljes
        ('set', -2),	# uutiset -> uutis
        ('ehkä', -1),	# ehkä -> ehk
        ('jotta', -1),	# jotta -> jott
        ('miksi', -1),	# miksi -> miks
        ('vaikka', -1),	# vaikka -> vaikk
        ('da', -2),     # voida -> voi
        ('dä', -2),     # entisöidä -> entisöi
        ('ttaa', -2),   # -ttaa -> -ta
        ('ttää', -2),   # -ttaa -> -ta
        ('ttua', -2),   # -ttua -> -tu
        ('ttyä', -2),   # -ttua -> -tu
        ('a', -1),      # for other verbal lemmas
        ('ä', -1),      # for other verbal lemmas
        ('imet', -2),   # elimet -> elin
        ('eet', -3),	# vaatteet -> vaate
        ('aat', -2),	# rattaat -> ratas
        ('t', -1),		# häät -> hää
    ]
    # Stem changes that affect the second (and possibly first) letter
    _stem_changes = [
        ('ie', 'ei'),
        ('uo', 'oi'),
        ('yö', 'öi'),
        ('aika', 'aja'),
        ('aika', 'ajo'),
        ('apu', 'avu'),
        ('etu', 'edu'),
        ('hyvä', 'paras'),
        ('hyvä', 'parem'),
        ('hyvä', 'parha'),
        ('ikä', 'iä'),
        ('itu', 'idu'),
        ('itä', 'idä'),
        ('oas', 'oka'),
        ('oka', 'oi'),
        ('ota', 'oda'),
        ('udar', 'utar'),
        ('uksi', 'ust'),
        ('ydin', 'ytim'),
        ('ei', 'em'),
        ('ei', 'en'),
        ('ei', 'et'),
    ]
    _vowels = 'aeiouyäö'
    # We would only need the second part if the first part can occur inflected
    # with some later part, such as "uudenkarhea", "uudenveroinen",
    # "vanhanmallinen". Another option might be to special-case them, as they
    # might be specific to certain later parts, such as "lainen", "puoleinen".
    # Capitalized (proper) nouns have been lowercased, as at least the UD1
    # variant of the Turku Dependency Parser Pipeline seems to lowercase some
    # of the lemmas ("tyynimeri").
    _inflecting_first_part = {
        ('aava', 'meri'),
        ('hieno', 'sokeri'),
        # ('Iso', 'Britannia'),
        ('iso', 'britannia'),
        ('iso', 'jako'),
        ('iso', 'koskelo'),
        ('iso', 'kuovi'),
        ('iso', 'käpylintu'),
        ('iso', 'lepinkäinen'),
        ('iso', 'lokki'),
        ('iso', 'maksaruoho'),
        ('iso', 'masto'),
        ('iso', 'panda'),
        ('iso', 'pistooli'),
        ('iso', 'purje'),
        ('iso', 'rokko'),
        ('iso', 'rumpu'),
        ('iso', 'sisar'),
        ('iso', 'sisko'),
        ('isot', 'aivot'),
        ('iso', 'varvas'),
        ('iso', 'veli'),
        ('iso', 'viha'),
        ('karkea', 'rehu'),
        ('kevyt', 'sarja'),
        ('kirjava', 'pillike'),
        ('kuiva', 'kakku'),
        ('kuiva', 'kukka'),
        ('kuiva', 'muona'),
        ('kuiva', 'paino'),
        ('kuiva', 'pari'),
        ('kuiva', 'rehu'),
        ('kultainen', 'noutaja'),
        ('laiska', 'koira'),
        ('lämmin', 'ruoka'),
        ('lämmin', 'varasto'),
        ('lämmin', 'vaunu'),
        ('matala', 'meri'),
        ('musta', 'herukka'),
        ('musta', 'leipä'),
        ('musta', 'leski'),
        ('musta', 'lintu'),
        ('musta', 'maija'),
        ('musta', 'makkara'),
        # ('Musta', 'meri'),
        ('musta', 'meri'),
        ('musta', 'mies'),
        ('musta', 'multa'),
        ('musta', 'pekka'),
        ('musta', 'pippuri'),
        ('musta', 'raamattu'),
        ('musta', 'torvisieni'),
        ('musta', 'viinimarja'),
        ('nuori', 'emäntä'),
        ('nuori', 'herra'),
        ('nuori', 'isäntä'),
        ('nuori', 'karja'),
        ('nuori', 'mies'),
        ('nuori', 'pari'),
        ('oma', 'kuva'),
        ('oma', 'tunto'),
        ('paha', 'henki'),
        ('paha', 'putki'),
        ('palava', 'kivi'),
        ('palava', 'pensas'),
        ('palava', 'rakkaus'),
        ('pitkä', 'housu'),
        ('pitkä', 'kirkko'),
        ('pitkä', 'perjantai'),
        ('pitkä', 'piimä'),
        ('pitkä', 'siima'),
        ('pitkä', 'takki'),
        ('pitkät', 'housut'),
        # ('Punainen', 'meri'),
        ('punainen', 'meri'),
        ('puoli', 'kuu'),
        ('puoli', 'matka'),
        ('puoli', 'päivä'),
        ('puoli', 'väli'),
        ('puoli', 'yö'),
        ('raitis', 'ilma'),
        ('raskas', 'sarja'),
        ('raskas', 'vesi'),
        ('raskas', 'vety'),
        ('sepivä', 'peippi'),
        ('suora', 'kulma'),
        ('suora', 'ommel'),
        ('syvä', 'meri'),
        ('särkynyt', 'sydän'),
        ('tyhjä', 'paino'),
        # ('Tyyni', 'meri'),
        ('tyyni', 'meri'),
        ('täysi', 'kuu'),
        # ('Uusi', 'kaupunki'),
        ('uusi', 'kaupunki'),
        ('uusi', 'kuu'),
        # ('Uusi', 'maa'),
        ('uusi', 'maa'),
        # ('Uusi', 'Seelanti'),
        ('uusi', 'seelanti'),
        ('uusi', 'vuosi'),
        # ('Uusi', '*'),
        # Does this overgenerate?
        ('uusi', '*'),
        ('vanha', 'emäntä'),
        ('vanha', 'isäntä'),
        ('vanha', 'kaupunki'),
        ('vanha', 'piika'),
        ('vanha', 'poika'),
        ('vieras', 'mies'),
    }
    _ordinal_lemmas = set(
        """ensimmäinen
           toinen
           kolmas
           neljäs
           viides
           kuudes
           seitsemäs
           kahdeksas
           yhdeksäs
           kymmenes
           sadas
           tuhannes
           miljoonas
           miljardis
           biljoonas
           toista"""
        .split())
    _numeral_lemmas = _ordinal_lemmas | set(
        """nolla
           yksi
           kaksi
           kolme
           neljä
           viisi
           kuusi
           seitsemän
           kahdeksan
           yhdeksän
           kymmenen
           sata
           tuhat
           miljoona
           miljardi
           biljoona
           kolmisen
           nelisen
           viitisen
           kuutisen
           seitsemisen
           kahdeksisen
           yhdeksisen
           kymmenisen
           pari"""
        .split())
    _powers10 = {
        'kymmenen': (1, 'kymmentä'),
        'sata': (2, 'sataa'),
        'tuhat': (3, 'tuhatta'),
        'miljoona': (6, 'miljoonaa'),
        'miljardi': (9, 'miljardia'),
        'biljoona': (12, 'biljoonaa'),
    }
    _adjust_case_fns = [
        (str.islower, str.lower),
        (str.isupper, str.upper),
        (str.istitle, str.title),
    ]

    def __init__(self):
        super().__init__()
        self._lemma_split_re = None
        self._lemma_split_keepsep_re = None
        self._omorfi_add_hyphens = False

    def main(self, args, inf, ouf):

        boundary_char = args.boundary_char.encode()
        fix_spurious_boundaries = (boundary_char == b'#'
                                   and not args.no_fix_spurious_boundaries)
        # Handle double boundary characters, which occur in for example
        # "neli||siipinen"
        self._lemma_split_re = re.compile('[' + args.boundary_char + '-]')
        self._lemma_split_keepsep_re = re.compile(
            '([' + args.boundary_char + '-]+)')
        self._omorfi_add_hyphens = not args.only_wordform_hyphens

        def make_lemma_noboundaries_simple(lemma, wordform):
            return lemma.replace(boundary_char, b'')

        # def log(before, kind, after):
        #     if args.verbose:
        #         print('#',
        #               *chain((w.decode().rstrip('\n')
        #                       for w in ([b'##'] + before)[-4:]),
        #                      [kind],
        #                      (w.decode().rstrip('\n')
        #                       for w in (after + [b'##'])[:6])),
        #               file=sys.stderr)

        def attr_index(s):
            """Convert numeric 1-based attribute index strings to 0-based ints,
            or return None for non-numeric."""
            try:
                return int(s) - 1
            except ValueError:
                return None

        LESS_THAN = '<'.encode()[0]
        if args.mode == 'omorfi':
            make_lemma_noboundaries = self._make_lemma_noboundaries_omorfi
        elif args.mode == 'naive':
            make_lemma_noboundaries = make_lemma_noboundaries_simple
        else:
            make_lemma_noboundaries = (
                self._make_lemma_noboundaries_omorfi_simple)
        for arg_name in ['word_attr', 'lemma_attr', 'nobound_attr',
                         'insert_after_attr']:
            setattr(args, arg_name, getattr(args, arg_name).encode())
        # print(args)
        word_index = attr_index(args.word_attr)
        lemma_index = attr_index(args.lemma_attr)
        insert_after_index = attr_index(args.insert_after_attr)
        if insert_after_index:
            insert_after_index += 1
        names_seen = False

        for line in inf:
            if line == b'\n':
                ouf.write(line)
            elif line[0] == LESS_THAN:
                if not names_seen and vrtnamelib.isbinnames(line):
                    word_index, lemma_index, insert_after_index = (
                        vrtnamelib.nameindices(vrtnamelib.binnamelist(line),
                                               args.word_attr,
                                               args.lemma_attr,
                                               args.insert_after_attr))
                    insert_after_index += 1
                    ouf.write(vrtnamelib.bininsertnames(
                        line, args.insert_after_attr, args.nobound_attr))
                else:
                    ouf.write(line)
            else:
                attrs = vrtdatalib.binasrecord(line)
                lemma = attrs[lemma_index]
                word = attrs[word_index]
                # If the word form contains a compound boundary marker, the
                # word is unlikely to be a real compound, so keep the lemma as
                # is.
                if boundary_char in lemma and not boundary_char in word:
                    # The Turku UD1 parser pipeline seems to convert literal
                    # |'s in lemmas to #'s if the wordform (and lemma) contains
                    # only punctuation characters
                    if (fix_spurious_boundaries
                            and lemma.count(boundary_char) == word.count(b'|')):
                        lemma_nobound = lemma.replace(boundary_char, b'|')
                    else:
                        lemma_nobound = make_lemma_noboundaries(lemma, word)
                else:
                    lemma_nobound = lemma
                ouf.write(b'\t'.join(chain(attrs[:insert_after_index],
                                           [lemma_nobound],
                                           attrs[insert_after_index:]))
                          + b'\n')

    def _make_lemma_noboundaries_omorfi_simple(self, lemma_b, wordform_b):
        # Adapted and slightly improved from vrt-fix-attrs.py
        # (PosAttrConverter._make_lemma_without_boundaries_tdt)
        lemma = lemma_b.decode()
        wordform = wordform_b.decode()
        boundary_char = self._args.boundary_char
        # If the boundary character is the first or the last in the lemma, it
        # should most likely be taken literally.
        if len(lemma) < 3 or boundary_char not in lemma[1:-1]:
            if (len(lemma) == 1 or boundary_char not in lemma
                    or (len(lemma) == 2 and lemma[0] == lemma[1])):
                return lemma_b
            # For example, "tuntinen" and "-tuntinen" get the lemma
            # "|tuntinen": in the first case, drop the boundary, in the second,
            # replace it with a hyphen.
            if lemma[0] == boundary_char:
                lemma = lemma[1:]
                if wordform[0] == '-':
                    lemma = '-' + lemma
            # Do trailing "|" occur?
            if lemma[-1] == boundary_char:
                lemma = lemma[:-1]
                if wordform[-1] == '-':
                    lemma += '-'
        elif '-' not in wordform:
            return lemma.replace(boundary_char, '').encode()
        # In some cases, the lemma has - replaced with a |; in
        # other cases not
        wordform_parts = wordform.split('-')
        lemmaparts = self._lemma_split_re.split(lemma)
        if (len(wordform_parts) == len(lemmaparts)
            and '-' not in lemma):
            return lemma.replace(boundary_char, '-').encode()
        else:
            lemma_without_boundaries = [lemmaparts[0]]
            lemma_prefix_len = len(lemmaparts[0])
            wf_prefix_len = len(wordform_parts[0])
            wf_partnr = 1
            for lemmapart in lemmaparts[1:]:
                if wf_partnr >= len(wordform_parts):
                    lemma_without_boundaries.append(lemmapart)
                elif (lemmapart[:2] == wordform_parts[wf_partnr][:2]
                      and abs(wf_prefix_len - lemma_prefix_len) <= 2):
                    # FIXME: Devise a better heuristic
                    lemma_without_boundaries.extend(['-', lemmapart])
                    wf_prefix_len += len(wordform_parts[wf_partnr])
                    wf_partnr += 1
                else:
                    lemma_without_boundaries.append(lemmapart)
                lemma_prefix_len += len(lemmapart)
            return ''.join(lemma_without_boundaries).encode()

    def _make_lemma_noboundaries_omorfi(self, lemma, wordform):

        def get_lemma_min_wf_len(lemma):
            for suffix, len_change in self._lemma_suffix_len_changes:
                if lemma.endswith(suffix):
                    return len(lemma) + len_change
            return len(lemma)

        lemma = lemma.decode()
        wordform = wordform.decode()
        # CHECK: Do we really need to preserve information on the separators,
        # because of possible hyphens?
        lemmaparts_boundaries = self._lemma_split_keepsep_re.split(lemma)
        lemmaparts = [part for partnum, part
                      in enumerate(lemmaparts_boundaries)
                      if partnum % 2 == 0]
        if all((lemmapart in self._numeral_lemmas
                or lemmapart.isdigit())
               for lemmapart in lemmaparts):
            return self._lemmatize_numeral(lemmaparts, wordform).encode()
        # If the word form has the same number of intra-word hyphens as the
        # lemma has boundaries (and hyphens), use the lemma of the last part
        # and the word forms of the preceding parts.
        if (lemma[1:-1].count(self._args.boundary_char) + lemma[1:-1].count('-')
                == wordform[1:-1].count('-')):
            result_parts = ([
                    self._adjust_case(wf_part, lemmaparts[partnum])
                    for partnum, wf_part
                    in enumerate(wordform.strip('-').split('-')[:-1])]
                ) + [lemmaparts[-1]]
            # If the second-last and last part form a compound in which the
            # second-last part inflects, use its lemma instead of the inflected
            # word form (e.g., nyky-Ison-Britannian -> nyky-Iso-Britannia). If
            # the inflected part is earlier, no changes are needed (e.g.,
            # Ison-Britannian-kävijät -> Ison-Britannian-kävijä).
            if self._has_inflecting_first_part(lemmaparts[-2], lemmaparts[-1]):
                result_parts[-2] = lemmaparts[-2]
            return '-'.join(result_parts).encode()
        wf_prefix_len = 0
        lemma_prefix_len = 0
        lemma_nobound = []
        # Calculate the minimum word form length covering a lemma part and the
        # parts following it
        min_rest_wf_lens = len(lemmaparts) * [0]
        rest_len = 0
        for partnum in range(len(lemmaparts) - 1, -1, -1):
            rest_len += get_lemma_min_wf_len(lemmaparts[partnum])
            min_rest_wf_lens[partnum] = rest_len
        for partnum in range(0, len(lemmaparts) - 1):
            part = lemmaparts[partnum]
            # Actual compound part
            # FIXME: end_diff and wf_use_len are not currently
            # used, and _get_prefix does not return meaningful
            # values for them
            nobound_part, end_diff, wf_use_len = self._get_prefix(
                lemmaparts[partnum:], wordform[wf_prefix_len:],
                min_rest_wf_lens[partnum + 1:])
            # print('after _get_prefix', partnum, part, nobound_part,
            #       end_diff, wf_use_len, file=sys.stderr)
            if (nobound_part and nobound_part[0] == '-'
                    and lemma_nobound and lemma_nobound[-1] == '-'):
                nobound_part = nobound_part[1:]
            lemma_nobound.append(nobound_part)
            # print(nobound_part, lemmaparts, partnum, file=sys.stderr)
            # nobound_part may be empty if the lemma begins with a boundary
            # marker, which has replaced a hyphen ("|tuntinen"). This
            # replaces such a boundary marker with a hyphen; an alternative
            # would be to leave it out if the word form has no initial
            # hyphen.
            if (self._omorfi_add_hyphens
                    and nobound_part and nobound_part[-1] != '-'
                    and lemmaparts[partnum + 1]
                    and nobound_part[-1] == lemmaparts[partnum + 1][0]
                    and nobound_part[-1] in self._vowels):
                lemma_nobound.append('-')
            wf_prefix_len += len(nobound_part)
            # print('lemma_nobound:', lemma_nobound, file=sys.stderr)
        lemma_nobound.append(lemmaparts[-1])
        return (''.join(lemma_nobound)).encode()

    def _get_adjust_case_fn(self, ref_word):
        for test_fn, adjust_fn in self._adjust_case_fns:
            if test_fn(ref_word):
                return adjust_fn
        return lambda x: x

    def _adjust_case(self, adjust_word, ref_word):
        return self._get_adjust_case_fn(ref_word)(adjust_word)

    def _has_inflecting_first_part(self, part1, part2):
        part1_lower = part1.lower()
        return ((part1_lower, part2.lower()) in self._inflecting_first_part
                or (part1_lower, '*') in self._inflecting_first_part)

    def _get_prefix(self, lemmaparts, wordform, min_rest_wf_lens):
        # min_rest_wf_len = the minimum number of (non-hyphen)
        # characters the word form should have left after the prefix
        # corresponding to lemmapart_this
        # yhteinen kunta yhteiskunnan -> yhteis, -2, 3
        # oleskella lupa oleskeluluvista ->
        lemmapart_this = lemmaparts[0]
        lemmapart_next = lemmaparts[1]
        # Inflecting first part should be considered only when the current part
        # is the second-last one, as for example, uudenvuodenlupauksia should
        # result in uudenvuodenlupaus, not uusivuodenlupaus.
        consider_inflecting_first_part = (len(lemmaparts) == 2)
        lemma_len = len(lemmapart_this)
        # print('_get_prefix', lemmapart_this, lemmapart_next, wordform,
        #       min_rest_wf_lens, consider_inflecting_first_part,
        #       file=sys.stderr)
        adjust_case_fn = self._get_adjust_case_fn(lemmapart_this)

        def find_begin(lemmapart_next):

            def common_suffix_len(a, b):
                ia = len(a) - 1
                ib = len(b) - 1
                while ia >= 0 and ib >= 0 and a[ia] == b[ib]:
                    ia -= 1
                    ib -= 1
                # print('CSL', a, b, '->', len(a) - ia - 1, file=sys.stderr)
                return len(a) - ia - 1

            def make_return_value(value):
                next_pos, wf_prefix_len, _ = value
                return (adjust_case_fn(wordform[:next_pos]),
                        next_pos - lemma_len, wf_prefix_len)

            def choose_candidate(cands):
                max_num = 0
                max_prefix_len = cands[0][1]
                max_suffix_len = cands[0][2]
                # print('max cand', max_num, max_prefix_len, max_suffix_len,
                #       file=sys.stderr)
                for cand_num, cand in enumerate(cands[1:]):
                    _, prefix_len, suffix_len = cand
                    # The candidates are ordered by the length of the common
                    # prefix for the word form part and the next lemma part, so
                    # prefer the longest common prefix, unless a shorter prefix
                    # results in a longer common substring consisting of the
                    # prefix and the suffix of the previous word form part and
                    # lemma part.
                    if (prefix_len + suffix_len > max_prefix_len + max_suffix_len):
                        max_num = cand_num + 1
                        max_prefix_len = prefix_len
                        max_suffix_len = suffix_len
                        # print('max cand', max_num, max_prefix_len, max_suffix_len,
                        #       file=sys.stderr)
                return cands[max_num]

            cands = []
            # Check word form prefix of 5...2 characters (or 1 if the length of
            # the next lemma part is 1)
            for wf_prefix_len in range(min(len(lemmapart_next), 5),
                                       min(len(lemmapart_next) - 1, 1), -1):
                # print('CHECK:', wordform, lemmapart_next, wf_prefix_len,
                #       lemmapart_next[:wf_prefix_len],
                #       wordform[(lemma_len - 3):], file=sys.stderr)
                next_pos = find_next_begin(wf_prefix_len, lemmapart_next)
                # print('next_pos', next_pos, file=sys.stderr)
                if next_pos:
                    for np in next_pos:
                        cands.append(
                            (np, wf_prefix_len,
                             common_suffix_len(lemmapart_this,
                                               wordform[:np].strip('-'))))
            # print('fnb cands', cands, file=sys.stderr)
            if not cands:
                return None
            elif len(cands) == 1:
                cand = cands[0]
            else:
                cand = choose_candidate(cands)
            return make_return_value(cand)

        def find_next_begin(wf_prefix_len, lemmapart_next):
            # print('find_next_begin', wf_prefix_len, lemmapart_next, lemma_len,
            #       file=sys.stderr)

            def all_parts_have_match(parts, whole):
                # Test if all the (lemma) parts in parts have a prefix
                # match of at least two characters in the (word form)
                # whole, so that each at each match, the remaining
                # word form is at least min_rest_wf_lens[partnum]
                # characters long.
                # TODO: Try to take into account possible stem changes.
                # print('all_parts_have_match', parts, whole,
                #       min_rest_wf_lens[1:], file=sys.stderr)
                pos = prev_pos = 0
                for partnum, part in enumerate(parts):
                    pos = whole.find(part[:2], prev_pos)
                    # print(partnum, part, pos)
                    if pos == -1 or (
                        	pos > len(whole) - min_rest_wf_lens[partnum + 1]):
                        return False
                    prev_pos = pos
                return True

            begin_cands = []
            lemma_len_change = 0
            for suffix, len_change in self._lemma_suffix_len_changes:
                if lemmapart_this.endswith(suffix):
                    lemma_len_change = len_change
                    break
            start_pos = max(0, lemma_len + lemma_len_change)
            next_pos = 0
            while start_pos < lemma_len + 3 and next_pos > -1:
                # print('fnb loop:', wordform, lemmapart_next[:wf_prefix_len],
                #       start_pos, wordform[start_pos:], file=sys.stderr)
                next_pos = wordform.find(lemmapart_next[:wf_prefix_len],
                                         start_pos)
                # print('next_pos', next_pos, file=sys.stderr)
                # FIXME: min_rest_wf_len does not take into account possible
                # hyphens in the word form.
                if 1 < next_pos <= len(wordform) - min_rest_wf_lens[0]:
                    if (begin_cands
                        and not all_parts_have_match(
                            lemmaparts[2:],
                            wordform[next_pos + wf_prefix_len:])):
                        # Avoid matching in maailmansodansotaveteraaneille :
                        # maa#ilman#sota#sota#veteraani the first lmma part
                        # "sota" with "sota" in the word form, as the second
                        # lemma part "sota" would then have no match in the
                        # word form.
                        break
                    # print('FOUND:', wf_prefix_len, next_pos, lemmapart_next,
                    #       wordform[:next_pos] + '['
                    #       + wordform[next_pos:(next_pos + wf_prefix_len)] + ']'
                    #       + wordform[next_pos + wf_prefix_len:],
                    #       file=sys.stderr)
                    begin_cands.append(next_pos)
                start_pos = next_pos + 1
            # print('begin_cands:', begin_cands, file=sys.stderr)
            if not begin_cands:
                return None
            elif len(begin_cands) == 1:
                return [begin_cands[0]]
            elif begin_cands[0] == 0 or wordform[begin_cands[0] - 1] == '-':
                # If the first candidate position is at the very beginning of
                # the word form or if it is preceded by a hyphen, prefer it.
                return [begin_cands[0]]
            else:
                # If the current lemma part ends with a suffix for which a
                # length change has been specified and the first candidate for
                # the next part start begins at or before the lemma with the
                # length adjusted, prefer that (suomalainen|tuttu,
                # suomalais<tu><tu>t -> suomalaistuttu and not
                # suomalaistututtu)
                if (lemma_len_change
                        and begin_cands[0] <= lemma_len + len_change):
                    return [begin_cands[0]]
                # Otherwise, choose the longest prefix with the last two
                # letters matching the previous lemma (eri|laki, eri<la>il<la>
                # -> erilaki and not erilaillaki)
                for i in range(len(begin_cands) - 1, -1, -1):
                    pos = begin_cands[i]
                    if pos >= 2 and wordform[pos - 2:pos] == lemmapart_this[-2:]:
                        return [pos]
                # Otherwise, return all and choose in
                # find_begin/choose_candidate.
                # Preferring the furthest match by reversing
                # begin_cands would fix some cases (naimisisiin :
                # naima#isä -> naimisisä instead of naimisä) but break
                # others (suuntaistahan : suu#tau -> suuntaistau).
                # begin_cands.reverse()
                return begin_cands

        # Are there other cases like ...toista, in which only the first part
        # inflects?
        if lemmapart_next.lower() == 'toista':
            return lemmapart_this, 0, len(lemmapart_this)
        # FIXME: We should take into account the consumed prefix of the
        # wordform and not look for lemmas there
        hyphen_pos = wordform.find('-', max(0, lemma_len - 3))
        # print(lemma_len, wordform, hyphen_pos, file=sys.stderr)
        maybe_hyphen = ('-' if hyphen_pos > -1 else '')
        if (consider_inflecting_first_part
                and self._has_inflecting_first_part(
                        lemmapart_this, lemmapart_next)):
            # print('INFL_FIRST', lemmapart_this + maybe_hyphen, 0,
            #       len(lemmapart_this), file=sys.stderr)
            return (lemmapart_this + maybe_hyphen, 0, len(lemmapart_this))
        return_vals = find_begin(lemmapart_next)
        if return_vals:
            return return_vals
        else:
            lemmapart_next_len = len(lemmapart_next)
            for lemma_stem, infl_stem in self._stem_changes:
                lemma_stem_len = len(lemma_stem)
                # print(lemma_stem, infl_stem, lemmapart_next_len,
                #       lemma_stem_len + 1, lemmapart_next.endswith(lemma_stem),
                #       file=sys.stderr)
                if (lemmapart_next_len <= lemma_stem_len + 1
                        and lemmapart_next.endswith(lemma_stem)):
                    return_vals = find_begin(
                        lemmapart_next[:(lemmapart_next_len - lemma_stem_len)]
                        + infl_stem)
                    if return_vals:
                        return return_vals
        # print('NOT FOUND', file=sys.stderr)
        return lemmapart_this, 0, 0

    def _lemmatize_numeral(self, lemmaparts, wordform):
        """Lemmatize compound numerals correctly based on compound parts.

        Omorfi lemmatizes all compound parts separately: for example,
        kahtasataa -> kaksi|sata. This function converts it to
        "kaksisataa". The wordform is used only for inserting hyphens
        at the correct places.
        """
        if all(lemmapart in self._ordinal_lemmas
               for lemmapart in lemmaparts[:-1]):
            if lemmaparts[-1] in self._ordinal_lemmas:
                return ''.join(lemmaparts)
            else:
                # Handle kymmenesmiljoonas : kymmenes#miljoona produced by
                # Omorfi. NOTE that this does *not* handle miljoonasbiljoonas :
                # miljoona#biljoona, as "miljoona" is not ordinal. It would
                # also be difficult to make it a special case and handle all
                # inflected forms.
                return ''.join(lemmaparts) + 's'
        wf_posthyphen_prefixes = re.findall(r'-(..)', wordform)
        hyphens_added = 0
        result = []
        for partnum, lemmapart in enumerate(lemmaparts):
            if (partnum > 0 and hyphens_added < len(wf_posthyphen_prefixes)
                    and lemmapart.startswith(
                        wf_posthyphen_prefixes[hyphens_added])):
                result.append('-')
                hyphens_added += 1
            if partnum > 0 and lemmapart in self._powers10:
                pow10, inflform = self._powers10[lemmapart]
                if lemmaparts[partnum - 1] in self._powers10:
                    prev_pow10, _ = self._powers10[lemmaparts[partnum - 1]]
                    if prev_pow10 > pow10:
                        result.append(lemmapart)
                        continue
                result.append(inflform)
            else:
                result.append(lemmapart)
        return ''.join(result)


if __name__ == '__main__':
    NoBoundaryLemmaAdder().run()
