#! /usr/bin/env python3
# -*- mode: Python; -*-

import codecs

from itertools import groupby, count
from queue import Queue
from subprocess import Popen, PIPE
from threading import Thread
import enum, os, re, sys, traceback

from vrtargslib import trans_args, trans_main
from vrtargslib import BadData, BadCode

from vrtnamelib import binxname, binxrest
from vrtnamelib import isbinnames as isnames
from vrtnamelib import binnamelist as namelist, nameindex, nameindices
from vrtnamelib import bininsertnames as insertnames

from vrtdatalib import binasrecord as asrecord
from vrtdatalib import binescape as escape, binunescape as unescape

CSTLEMMA = [
    '/proj/kieli/sparv/cstlemma',
    
    # input is UTF-8-encoded, tagged, word TAB tag NL
    '-eU',
    '-t',
    '-I', r'$w\t$t\n',

    # output is word TAB info TAB dictionary lemma TAB rule-based lemma NL
    # and lemmas turned out to be *sometimes* not UTF-8 (but Latin-1?)
    # NEEDS INVESTIGATED BUT WORKAROUNDIT FOR NOW;
    #
    # From cstlemma -h for the option -c:
    #    $i info:  -    full form not in dictionary.
    #              +    full form has more than one lemma in the dictionary.
    #                   (If that is the case, the lemmatiser also generates lemma(s)
    #                   using the flex pattern rules, so you can choose.)
    #           (blank) full form in dictionary.
    '-c', '$w\t$i\t$b\t$B\n',
    '-b', '$w',
    '-B', '$w',
]

MODELS = dict(
    saldo = [
        '-d', '/proj/kieli/sparv/models/saldo.cstlemma.dict', # dictionary
        '-f', '/proj/kieli/sparv/models/saldo.cstlemma.flex', # rules
    ],
    suc = [
        '-d', '/proj/kieli/sparv/models/suc.cstlemma.dict', # dictionary
        '-f', '/proj/kieli/sparv/models/suc.cstlemma.flex', # rules
    ],
    sucsaldo = [
        '-d', '/proj/kieli/sparv/models/suc-saldo.cstlemma.dict', # dictionary
        '-f', '/proj/kieli/sparv/models/suc-saldo.cstlemma.flex', # rules
    ]

)

def parsearguments():
    description = '''

    Pass word forms and specially prepared features in a flat vrt
    document through cstlemma using models based on the tagged saldo
    and suc word lists in the sparv distribution. Insert after each
    word a dictionary-match indicator (lemma.match: full, form, none),
    a dictionary lemma (lemma.dict), and a rule-derived lemma
    (lemma.flex). (TODO: pick one as lemma, and those may be
    multivalued?)

    '''

    parser = trans_args(description = description)
    parser.add_argument('--word', '-w', metavar = 'name',
                        type = binxname, default = b'word',
                        help = 'input word field name (default word)')
    parser.add_argument('--tag', '-t', metavar = 'name',
                        type = binxname, default = b'msd',
                        help = 'input tag field name (default msd)')

    # so the defaults are not valid values of the type?
    parser.add_argument('--prefix', '-p', metavar = 'fix',
                        type = binxname, default = b'',
                        help = 'prefix to output field names')
    parser.add_argument('--suffix', '-s', metavar = 'fix',
                        type = binxrest, default = b'',
                        help = 'suffix to output field names')

    parser.add_argument('--model',
                        choices = [ 'saldo', 'suc', 'sucsaldo' ],
                        default = 'suc',
                        help = '''

                        lemmatisation models to use (saldo is based on
                        a huge list, suc on a short list)

                        ''')
    parser.add_argument('--strict', action = 'store_true',
                        help = '''

                        highlight all invalid bytes in lemmas as
                        "[UTF-8?xx]" (default is to interpret hex
                        a0-ff as Latin-1)

                        ''')

    args = parser.parse_args()
    args.prog = parser.prog
    return args

def message(args, mess):
    print(args.prog + ':', mess, file = sys.stderr)

def terminate(proc):
    try:
        proc.terminate()
    except ProcessLookupError:
        pass

def highlight_errors(exn):
    '''Handles UnicodeDecodeError. Because some but not all occurrences of
    the usual letters are mal-encoded in the output of cstlemma.

    '''

    bad = (
        codecs
        .encode(exn.object[exn.start:exn.end], 'hex')
        .decode('ASCII')
    )
    return '[UTF-8?{}]'.format(bad), exn.end

def interpret_errors(exn):
    '''Handles UnicodeDecodeError. Because some but not all occurrences of
    the usual letters are mal-encoded in the output of cstlemma.
    Interpret them in Latin-1.

    '''

    bad = exn.object[exn.start:exn.end]

    # only handle graphic characters
    if all(b > 0xa0 for b in bad):
        return bad.decode('iso-8859-1'), exn.end

    return highlight_errors(exn)

codecs.register_error('highlight', highlight_errors)
codecs.register_error('interpret', interpret_errors)

def utf8(args, word):
    '''Bring the word to this millennium.'''

    if args.strict:
        word = word.decode('UTF-8', errors = 'highlight')
    else:
        word = word.decode('UTF-8', errors = 'interpret')

    return word.encode('UTF-8')

def main(args, inf, ouf):

    with Popen(CSTLEMMA + MODELS[args.model],
               stdin = PIPE,
               stdout = PIPE,
               stderr = sys.stderr.buffer) as cstlemma:

        copy = Queue()
        Thread(target = combine, args = (args, cstlemma, copy, ouf)).start()

        status = 1
        try:
            implement_main(args, inf, cstlemma, copy)
            status = 0
        except BadData as exn:
            message(args, exn)
        except BrokenPipeError as exn:
            message(args, 'broken pipe in main thread')
        except KeyboardInterrupt as exn:
            message(args, 'keyboard interrupt in main thread')
        except Exception as exn:
            print(traceback.format_exc(), file = sys.stderr)

        if status:
            terminate(cstlemma)
        else:
            cstlemma.stdin.close()

        try:
            copy.join()
        except KeyboardInterrupt:
            message(args, 'keyboard interrupt in main thread')
            status = 1

        return status

def implement_main(args, inf, cstlemma, copy):

    # each "word" and "tag" go to cstlemma, with an empty line (EXCEPT
    # IT CANNOT BE AN EMPTY LINE BECAUSE THOSE VANISH) after each
    # sentence; everything goes to copy in alternative groups of meta
    # and data, with new "dict.lemma", "rule.lemma" (or such) in names

    wordix, posix = None, None

    def send(sentence):
        for record in sentence:
            cstlemma.stdin.write(unescape(record[wordix]))
            cstlemma.stdin.write(b'\t')
            cstlemma.stdin.write(unescape(record[posix]))
            cstlemma.stdin.write(b'\n')
        else:
            cstlemma.stdin.write(b'<< >>\t_\n')
            cstlemma.stdin.flush()

    def setnames(line):
        nonlocal wordix, posix
        if isnames(line):
            wordix, posix = nameindices(namelist(line), args.word, args.tag)
            return insertnames(line, args.word,
                               args.prefix + b'lemma.match' + args.suffix,
                               args.prefix + b'lemma.dict' + args.suffix,
                               args.prefix + b'lemma.flex' + args.suffix)
        return line

    def issome(line): return not line.isspace()
    def ismeta(line): return line.startswith(b'<')

    first = True
    for groupismeta, group in groupby(filter(issome, inf), ismeta):

        if groupismeta:
            meta = tuple(map(setnames, group))
            copy.put(meta)
            first = False
            continue

        # groupisdata

        if first:
            # there shall always be previous meta
            copy.put(())
            first = False

        if wordix is None:
            raise BadData('error: token before field names')

        sentence = tuple(map(asrecord, group))
        copy.put(sentence)
        send(sentence)

    if not groupismeta:
        # there shall always be final meta
        copy.put(())

def combine(args, cstlemma, copy, out):
    '''Read cstlemma output (word TAB lemma TAB lemma NL) and flat vrt
    from the copy process. Insert lemmas from cstlemma to the vrt at
    the named position.

    This is run as a thread that consumes the cstlemma process and
    syncs it with the copy queue. Preceding meta and corresponding
    data were put in the queue before the data was sent to cstlemma,
    so they will always be there when a sentence is read out of
    cstlemma.

    '''

    fail = True
    try:
        implement_combine(args, cstlemma, copy, out)
        fail = False
    except BrokenPipeError:
        message(args, 'broken pipe in combine thread')
    except StopIteration:
        # not sure when this can happen now
        message(args, 'stop iteration in combine thread')
    except ValueError as exn:
        # sometimes keyboard interruption in main thread produces here
        # a readline of closed file (or at least it did in one stage
        # in development)
        message(args, 'value error in combine thread ' + str(exn))
    finally:
        if fail: terminate(cstlemma)

def implement_combine(args, cstlemma, copy, out):
    '''Thread may find pipe closed.'''

    def issentinel(line): return line.startswith(b'<< >>\t')
    it = {
        b'-' : b'none', # full form not in dictionary
        b'+' : b'many', # has many lemmas in dictionary
        b' ' : b'one', # update: "(blank)" seems to be this one!
        b'' : b'one', # should not happen, nor any other
    }

    response = (tokens
                for isempty, tokens
                in groupby(cstlemma.stdout, issentinel)
                if not isempty)

    at = None # word field index, after which insert new
    for analyses in response:

        meta = copy.get_nowait()
        data = copy.get_nowait()
        copy.task_done()
        copy.task_done()

        for line in meta:
            if isnames(line):
                at = nameindex(namelist(line), args.word)

        if at is None:
            raise BadData('combine thread: data before names')

        shipmeta(meta, out)
        for new, old in zip(analyses, data):
            [
                word,
                info,
                dilemma,
                rulemma
            ] = asrecord(new)

            old.insert(at + 1, escape(utf8(args, rulemma)) or b'_')
            old.insert(at + 1, escape(utf8(args, dilemma)) or b'_')
            old.insert(at + 1, it.get(info, info)) # b'todo')
            
        else:
            shipdata(data, out)

    # should this have a timeout? or could it be .get_nowait()?
    # added flush and made get get_nowait because a few processes
    # seemed to make no progress till they timed out - were they
    # stalled here?
    out.flush()
    shipmeta(copy.get_nowait(), out)
    copy.task_done()

def shipmeta(meta, out):
    for line in meta: out.write(line)

from datetime import datetime
def shipdata(data, out):
    if len(data) > 2000:
        # [this is a remnant from the marmot tool in td pipeline]
        # where even marmot appeared to behave badly with long
        # "sentences" (though only much longer than the parser -
        # present limit of 2000 is taken out of thin air and could
        # probably have been much larger but then it only controls a
        # warning in stderr)
        sys.stderr.buffer.write(b'shipping data at ')
        sys.stderr.buffer.write(datetime.now().isoformat().encode())
        sys.stderr.buffer.write(b' of len ')
        sys.stderr.buffer.write(str(len(data)).encode())
        sys.stderr.buffer.write(b'\n')
        sys.stderr.buffer.flush()
    for record in data:
        out.write(b'\t'.join(record))
        out.write(b'\n')

if __name__ == '__main__':
    trans_main(parsearguments(), main,
               in_as_text = False,
               out_as_text = False)
